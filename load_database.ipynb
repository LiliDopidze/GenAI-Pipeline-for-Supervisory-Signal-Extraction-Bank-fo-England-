{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1-9i49Cncvm8a6xe5bOm6WrTE3zFD5arQ","timestamp":1757009313915}],"collapsed_sections":["hnd40vSnlE_g"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Environment Setup"],"metadata":{"id":"SJ0lF0ydmIoV"}},{"cell_type":"code","source":["# Install necessary Python packages\n","!pip install -q python-docx google-api-python-client faiss-cpu"],"metadata":{"id":"fkgLHcV5nqyl","executionInfo":{"status":"ok","timestamp":1759687489855,"user_tz":-60,"elapsed":18838,"user":{"displayName":"Alvaro Fernandez","userId":"16889755110417313838"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"e0e830a8-a265-43f0-a192-eb972dc28bad"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.0/253.0 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.4/31.4 MB\u001b[0m \u001b[31m36.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}]},{"cell_type":"code","source":["# Import necessary libraries\n","\n","# Standard library\n","import os, io, re, json, math\n","from json import JSONDecodeError\n","from dataclasses import dataclass\n","from typing import Any, Dict\n","\n","# Third-party libraries\n","import numpy as np\n","import pandas as pd\n","import faiss\n","from tqdm import tqdm\n","from docx import Document\n","\n","# Google Colab & Drive API\n","from google.colab import auth\n","from oauth2client.client import GoogleCredentials\n","from googleapiclient.discovery import build\n","from googleapiclient.http import MediaIoBaseDownload, MediaFileUpload"],"metadata":{"id":"wVp4PkfCli3t"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[" # Drive Utilities and Global Configuration"],"metadata":{"id":"HapgdJhuS6xk"}},{"cell_type":"code","source":["# [Global Config + Drive Session]\n","# ---------------------------------------------------------------------\n","# Centralised configuration and reusable Google Drive client/session.\n","# - Define filenames and folder IDs used across the notebook.\n","# - Provide small, focused helpers for Drive lookups and downloads.\n","# - Cache the Drive service so we authenticate only once.\n","# ---------------------------------------------------------------------\n","\n","# --- Global configuration -----------------------------------------------------\n","DATABASE_FOLDER_ID      = \"1AxyCDytXdUViBaOAT5dyQbOmbECIqS4C\"\n","DATABASE_FILENAME       = \"database_df.jsonl\"\n","CHUNKS_FILENAME         = \"chunks.jsonl\"\n","DEMBEDDINGS_FILENAME    = \"dense_embeddings.npy\"\n","FAISS_FILENAME          = \"faiss.index\"\n","\n","# --- Drive session (cached) ---------------------------------------------------\n","drive = None\n","\n","def _drive():\n","  \"\"\"Return a cached Google Drive v3 client, authenticating on first use.\"\"\"\n","  global drive\n","  if drive is None:\n","    auth.authenticate_user()\n","    creds = GoogleCredentials.get_application_default()\n","    drive = build(\"drive\", \"v3\", credentials=creds)\n","  return drive\n","\n","def _find_file(drive, folder_id: str, name: str):\n","  \"\"\"Find a file by exact name within a folder; return its file ID or None.\"\"\"\n","  q = f\"'{folder_id}' in parents and trashed=false and name='{name}'\"\n","  res = drive.files().list(q=q, fields=\"files(id,name)\", pageSize=1).execute()\n","  files = res.get(\"files\", [])\n","  return files[0][\"id\"] if files else None\n","\n","def _download_file_to_path(drive, file_id: str, path: str):\n","  \"\"\"Download a Drive file (by ID) to a local filesystem path.\"\"\"\n","  req = drive.files().get_media(fileId=file_id)\n","  with open(path, \"wb\") as fh:\n","    downloader = MediaIoBaseDownload(fh, req)\n","    done = False\n","    while not done:\n","      _, done = downloader.next_chunk()\n","\n","def _download_file_to_bytes(drive, file_id: str) -> bytes:\n","  \"\"\"Download a Drive file (by ID) fully into memory and return raw bytes.\"\"\"\n","  buf = io.BytesIO()\n","  req = drive.files().get_media(fileId=file_id)\n","  downloader = MediaIoBaseDownload(buf, req)\n","  done = False\n","  while not done:\n","    _, done = downloader.next_chunk()\n","  return buf.getvalue()"],"metadata":{"id":"dgRi_xCZS9Ox"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Load Core Database"],"metadata":{"id":"hnd40vSnlE_g"}},{"cell_type":"code","source":["# [Load Database (JSONL Only) into database_df]\n","# ---------------------------------------------------------------------\n","# Loads a JSONL dataset from Google Drive into a pandas DataFrame.\n","# - Robust JSONL reader with a fallback line-by-line parser.\n","# - Returns a DataFrame with at least: [\"filename\", \"content\", \"preprocessed\"].\n","# ---------------------------------------------------------------------\n","\n","# -----------------------\n","# JSONL I/O\n","# -----------------------\n","def _read_jsonl_df(path: str) -> pd.DataFrame:\n","  \"\"\"Read a JSONL file into a DataFrame (fallback parser on error).\n","\n","  Ensures the columns [\"filename\", \"content\", \"preprocessed\"] exist.\n","  \"\"\"\n","  if not os.path.exists(path):\n","    return pd.DataFrame(columns=[\"filename\", \"content\", \"preprocessed\"])\n","\n","  # Fast path: pandas JSONL reader\n","  try:\n","    df = pd.read_json(path, orient=\"records\", lines=True, dtype=False)\n","  except ValueError:\n","    # Fallback: line-by-line robust reader\n","    rows = []\n","    with open(path, \"r\", encoding=\"utf-8\") as f:\n","      for line in f:\n","        s = line.strip()\n","        if not s:\n","          continue\n","        try:\n","          rows.append(json.loads(s))\n","        except JSONDecodeError:\n","          continue\n","    df = pd.DataFrame(rows)\n","\n","  # Ensure required columns exist\n","  for col in (\"filename\", \"content\", \"preprocessed\"):\n","    if col not in df.columns:\n","      df[col] = None\n","\n","  return df\n","\n","# -----------------------\n","# Coerce temporal fields\n","# -----------------------\n","def _coerce_temporal_fields(df: pd.DataFrame) -> pd.DataFrame:\n","  \"\"\"Normalize year and quarter columns in-place to numeric types.\"\"\"\n","  if df.empty:\n","    return df\n","\n","  df = df.copy()\n","\n","  df[\"year\"] = (\n","    pd.to_numeric(df.get(\"year\"), errors=\"coerce\")\n","      .astype(\"Int64\")\n","  )\n","\n","  quarter_numbers = (\n","    df.get(\"quarter\")\n","      .astype(str)\n","      .str.extract(r\"Q([1-4])\", expand=False)\n","  )\n","  df[\"quarter\"] = pd.to_numeric(quarter_numbers, errors=\"coerce\").astype(\"Int64\")\n","\n","  return df\n","\n","# -----------------------\n","# Load database\n","# -----------------------\n","def load_database():\n","  \"\"\"Locate JSONL in Drive, download locally, read into a sorted DataFrame.\"\"\"\n","  drive = _drive()\n","  local_tmp = f\"/content/{DATABASE_FILENAME}\"\n","\n","  jsonl_id = _find_file(drive, DATABASE_FOLDER_ID, DATABASE_FILENAME)\n","  if not jsonl_id:\n","    raise FileNotFoundError(f\"'{DATABASE_FILENAME}' not found in folder {DATABASE_FOLDER_ID}.\")\n","\n","  _download_file_to_path(drive, jsonl_id, local_tmp)\n","  df = _read_jsonl_df(local_tmp)\n","  df = _coerce_temporal_fields(df)\n","\n","  if not df.empty and \"filename\" in df.columns:\n","    df = df.sort_values(by=\"filename\", ascending=False).reset_index(drop=True)\n","\n","  return df\n","\n","# -----------------------\n","# Entrypoint\n","# -----------------------\n","database_df = load_database()"],"metadata":{"id":"NFxAQ-1wXNYT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Load Chunk Artefacts"],"metadata":{"id":"Isf9Jfr56Cca"}},{"cell_type":"code","source":["# [Load Chunk Artefacts]\n","# ---------------------------------------------------------------------\n","# Loads precomputed chunks from Google Drive (JSONL) and materialises\n","# them as a list of Chunk dataclass instances.\n","# ---------------------------------------------------------------------\n","\n","@dataclass\n","class Chunk:\n","  \"\"\"Atomic transcript unit for retrieval/embedding.\"\"\"\n","  chunk_id: str\n","  text: str\n","  filename: str\n","  doc_id: str\n","  speaker: str\n","  row_index: int\n","  block_index: int\n","  metadata: Dict[str, Any]\n","\n","def _as_int(value):\n","  if isinstance(value, (int, float)) and not math.isnan(value):\n","    return int(value)\n","  if isinstance(value, str) and value.isdigit():\n","    return int(value)\n","  return None\n","\n","def load_chunks() -> list[Chunk]:\n","  local_tmp = f\"/content/{CHUNKS_FILENAME}\"\n","  file_id = _find_file(_drive(), DATABASE_FOLDER_ID, CHUNKS_FILENAME)\n","  if not file_id:\n","    raise FileNotFoundError(f\"{CHUNKS_FILENAME} not found in Drive folder\")\n","\n","  _download_file_to_path(_drive(), file_id, local_tmp)\n","  df = _read_jsonl_df(local_tmp)\n","\n","  ordered: list[tuple[int | None, Chunk]] = []\n","  for row in df.to_dict(\"records\"):\n","    meta = row.get(\"metadata\", {}) or {}\n","    if isinstance(meta, str):\n","      try:\n","        meta = json.loads(meta)\n","      except Exception:\n","        meta = {}\n","\n","    embed_idx = _as_int(meta.get(\"embedding_row\"))\n","\n","    chunk = Chunk(\n","      chunk_id    = row.get(\"chunk_id\", \"\"),\n","      text        = row.get(\"text\", \"\") or \"\",\n","      filename    = row.get(\"filename\", \"\") or \"\",\n","      doc_id      = row.get(\"doc_id\", \"\") or \"\",\n","      speaker     = row.get(\"speaker\", \"\") or \"\",\n","      row_index   = int(row.get(\"row_index\", -1)) if row.get(\"row_index\") is not None else -1,\n","      block_index = int(row.get(\"block_index\", -1)) if row.get(\"block_index\") is not None else -1,\n","      metadata    = meta if isinstance(meta, dict) else {},\n","    )\n","    ordered.append((embed_idx, chunk))\n","\n","  max_idx = max((idx for idx, _ in ordered if idx is not None), default=-1)\n","  slots = [None] * (max_idx + 1)\n","  tail: list[Chunk] = []\n","\n","  for idx, chunk in ordered:\n","    if idx is not None and idx >= 0:\n","      if idx >= len(slots):\n","        slots.extend([None] * (idx - len(slots) + 1))\n","      slots[idx] = chunk\n","    else:\n","      tail.append(chunk)\n","\n","  chunks_list = [c for c in slots if c is not None] + tail\n","  return chunks_list\n","\n","# Entrypoint\n","chunks = load_chunks()"],"metadata":{"id":"QyLoQWjO6Ao5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#  Load Dense Retrieval Artefacts"],"metadata":{"id":"0TbLNP8VJ1zO"}},{"cell_type":"code","source":["# [Load Dense Embeddings from Drive]\n","# ---------------------------------------------------------------------\n","# Downloads the dense embedding matrix (.npy) from Google Drive and loads\n","# it into memory as a NumPy array for use in dense retrieval or FAISS search.\n","# ---------------------------------------------------------------------\n","\n","file_id = _find_file(_drive(), DATABASE_FOLDER_ID, DEMBEDDINGS_FILENAME)\n","if not file_id:\n","    raise FileNotFoundError(f\"{DEMBEDDINGS_FILENAME} not found in folder {DATABASE_FOLDER_ID}\")\n","\n","dense_embeddings = np.load(io.BytesIO(_download_file_to_bytes(_drive(), file_id)), allow_pickle=False)"],"metadata":{"id":"Oad359rENEox"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# [Load FAISS Index from Drive]\n","# ---------------------------------------------------------------------\n","# Downloads the FAISS index binary from Google Drive and deserialises it\n","# into a FAISS index object for dense retrieval.\n","# ---------------------------------------------------------------------\n","\n","file_id = _find_file(_drive(), DATABASE_FOLDER_ID, FAISS_FILENAME)\n","if not file_id:\n","    raise FileNotFoundError(f\"{FAISS_FILENAME} not found in Drive folder\")\n","\n","payload = np.frombuffer(_download_file_to_bytes(_drive(), file_id), dtype=np.uint8)\n","faiss_index = faiss.deserialize_index(payload)"],"metadata":{"id":"xP_oMgCBJ50W"},"execution_count":null,"outputs":[]}]}